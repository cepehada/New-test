"""
Парсер CoinDesk.
Парсит новости с сайта CoinDesk.
"""

import logging
from bs4 import BeautifulSoup
from project.utils.http_utils import async_get
from typing import List, Dict, Any, Optional
from datetime import datetime
from project.news.parsers.base_parser import NewsParser  # Новый импорт

logger = logging.getLogger("CoinDeskParser")

# Конфигурация для парсера
COINDESK_CONFIG = {
    "base_url": "https://www.coindesk.com/",
    "selectors": {
        "article": "article.article-default",
        "title": "h1.heading",
        "content": ".article-body p",
        "link": "a",
        "timestamp": "time"
    }
}


class CoinDeskParser(NewsParser):
    """Парсер для новостного сайта CoinDesk"""
    
    def __init__(self):
        super().__init__(source_name="coindesk", base_url=COINDESK_CONFIG["base_url"])
        self.selectors = COINDESK_CONFIG["selectors"]
    
    async def fetch_articles(self, limit: Optional[int] = 10) -> List[Dict[str, Any]]:
        """
        Получает статьи с CoinDesk.
        
        Args:
            limit: Максимальное количество статей для получения
            
        Returns:
            Список словарей со статьями
        """
        articles = []
        try:
            html = await async_get(self.base_url)
            if not html:
                return articles
                
            raw_articles = self._parse_html(html)
            
            # Ограничиваем количество статей
            limited_articles = raw_articles[:limit] if limit else raw_articles
            
            # Стандартизируем формат
            for article in limited_articles:
                articles.append(self.standardize_article(article))
                
        except Exception as e:
            self.logger.error(f"Ошибка при получении статей с {self.base_url}: {e}")
        
        return articles
    
    def _parse_html(self, html: str) -> List[Dict]:
        """
        Парсит HTML-контент сайта CoinDesk.
        
        Args:
            html: HTML-контент страницы
            
        Returns:
            Список словарей со статьями
        """
        soup = BeautifulSoup(html, "html.parser")
        articles = []
        
        for art in soup.select(self.selectors["article"]):
            try:
                title = art.select_one(self.selectors["title"]).text.strip()
                url = art.find(self.selectors["link"])["href"]
                
                # Если URL относительный, добавляем базовый URL
                if url and not url.startswith(('http://', 'https://')):
                    url = self.base_url.rstrip('/') + '/' + url.lstrip('/')
                
                # Собираем текст из всех параграфов
                content = "\n".join([p.text for p in art.select(self.selectors["content"])])
                
                # Получаем временную метку
                timestamp_elem = art.select_one(self.selectors["timestamp"])
                ts = timestamp_elem["datetime"] if timestamp_elem else datetime.now().isoformat()
                
                articles.append({
                    "source": self.source_name,
                    "title": title,
                    "url": url,
                    "content": content,
                    "timestamp": ts,
                })
            except Exception as e:
                self.logger.error(f"Ошибка при парсинге статьи CoinDesk: {e}")
        
        return articles